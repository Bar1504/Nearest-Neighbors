{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c277bc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import time\n",
    "from PIL import Image, ImageDraw\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "hananya1=pd.read_csv('Hananya1.csv')\n",
    "hananya2=pd.read_csv('Hananya2.csv') \n",
    "hashmal1=pd.read_csv('Hashmal1.csv')\n",
    "hashmal2=pd.read_csv('Hashmal2.csv')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class KDTreeNode:\n",
    "    def __init__(self, data, left=None, right=None, split_dim=None, split_val=None):\n",
    "        self.data = data\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.split_dim = split_dim\n",
    "        self.split_val = split_val\n",
    "\n",
    "class RandomizedKDTree:\n",
    "    def __init__(self, leaf_size):\n",
    "        self.root = None\n",
    "        self.leaf_size = leaf_size\n",
    "\n",
    "    def build_kd_tree(self, arr):\n",
    "        self.root = self._randomized_kd_tree(arr)\n",
    "\n",
    "    def _randomized_kd_tree(self, arr):\n",
    "        if len(arr) <= self.leaf_size:\n",
    "            arr=arr.astype(np.float64)\n",
    "            return KDTreeNode(arr)\n",
    "        \n",
    "        n, m = arr.shape\n",
    "        # Randomly select split dimension out of the 128 relevant features\n",
    "        split_dim = np.random.randint(4,m-1)\n",
    "        # Find the median value of the selected dimension\n",
    "        median = np.median(arr[:, split_dim])\n",
    "        #Split the data to 2 groups by the median\n",
    "        median_idx = len(arr) // 2\n",
    "        points_sorted = arr[np.argsort(arr[:, split_dim])]\n",
    "        median_point = points_sorted[median_idx]\n",
    "        left_points = points_sorted[:median_idx]\n",
    "        right_points = points_sorted[median_idx+1:]\n",
    "        \n",
    "        \n",
    "        left = self._randomized_kd_tree(left_points)\n",
    "        right = self._randomized_kd_tree(right_points)\n",
    "\n",
    "        return KDTreeNode(median, left, right, split_dim, median)\n",
    "\n",
    "    def construct_tree_from_dataframe(self, dataframe):\n",
    "        # Convert DataFrame to NumPy array \n",
    "        data = dataframe.to_numpy()\n",
    "        # Call the _randomized_kd_tree function\n",
    "        self.build_kd_tree(data)\n",
    "    \n",
    "    #By a given record\\query point this function find the leaf in the tree that point belongs\n",
    "                \n",
    "    def find_leaf_node(self, point):\n",
    "        current_node = self.root\n",
    "        while current_node.left or current_node.right:\n",
    "            if point[current_node.split_dim] < current_node.split_val:\n",
    "                if current_node.left:\n",
    "                    current_node = current_node.left\n",
    "                else:\n",
    "                    break\n",
    "            else:\n",
    "                if current_node.right:\n",
    "                    current_node = current_node.right\n",
    "                else:\n",
    "                    break\n",
    "        return current_node\n",
    "    \n",
    "class ANN:\n",
    "    \n",
    "    #The implementation of both RKDT and LSH algorithm \n",
    "    #L=number of trees\\tables\n",
    "    #leaf_size=the maximum amount of points the a leaf can has(RKDT)\n",
    "    #num_cuts=the number of dimantions we use to split the data(LSH)\n",
    "    \n",
    "    def __init__(self,algorithm,L,leaf_size=None, num_cuts=None):\n",
    "        self.algo=algorithm\n",
    "        if self.algo=='RKDT':\n",
    "            self.L = L\n",
    "            self.leaf_size = leaf_size\n",
    "            self.trees=[]\n",
    "        if self.algo=='LSH':\n",
    "            self.L=L\n",
    "            self.num_cuts=num_cuts\n",
    "            self.tables=[]\n",
    "            \n",
    "    #Data contains the relevant features (in our case the 128 features)\n",
    "    #Target contains the label of the records(in our case x,y of every point)\n",
    "    # initialize the self.target variable only for the classification\n",
    "    def fit(self,X,y=None):\n",
    "        self.data=X\n",
    "        self.target=y\n",
    "        #add the coloumn 'index' to the data-set to keep the record number of the original data set for every point\n",
    "        self.data=self.data.reset_index()\n",
    "        index_col = self.data.pop(\"index\")  \n",
    "        self.data[\"index\"] = index_col\n",
    "        #Create trees\\tables to train the data-set\n",
    "        if self.algo=='RKDT':\n",
    "            for i in range(self.L):\n",
    "                tree=RandomizedKDTree(self.leaf_size)\n",
    "                tree.construct_tree_from_dataframe(self.data)\n",
    "                self.trees.append(tree)\n",
    "        if self.algo=='LSH':\n",
    "            for i in range(self.L):\n",
    "                lsh=LSH(self.num_cuts,self.data)\n",
    "                self.tables.append(lsh)\n",
    "    \n",
    "    #By a given query_point this method provides th top-k nearest neighbores of the point by calculating the distances\n",
    "    def find_k_neighbores(self,query_point,k):\n",
    "        \n",
    "        #set_tuples=Set that contains tuples((x,y),distance,index) of the k nearest points of every tree\\table.\n",
    "        # use SET to avoid adding the same point over and over again to the data-structure\n",
    "        set_tuples=set()\n",
    "        #sorted_tuples=Sorted List of the top k nearest points from ALL trees\\tables \n",
    "        sorted_tuples=[]\n",
    "        \n",
    "        \n",
    "        \n",
    "        query_point=query_point.to_numpy().astype(float)\n",
    "        if self.algo=='RKDT':\n",
    "            for tree in self.trees:\n",
    "                leaf=tree.find_leaf_node(query_point)\n",
    "                self.find_closeset(set_tuples,leaf.data,query_point,k)\n",
    "\n",
    "            sorted_tuples = sorted(set_tuples, key=lambda x: x[1])\n",
    "\n",
    "            return sorted_tuples[:k]\n",
    "        \n",
    "        if self.algo=='LSH':\n",
    "            for table in self.tables:\n",
    "                cut=table.find_table(query_point)\n",
    "\n",
    "                self.find_closeset(set_tuples,cut,query_point,k)\n",
    "                \n",
    "            #This \"IF\" deal with the case of empty cuts in every table. because of the fact that this is a rare situation we find the k nearest point by KNN algorithm     \n",
    "            if len(set_tuples)==0:\n",
    "                \n",
    "                knn=KNN()\n",
    "                knn.fit(self.data)\n",
    "                query_to_data = pd.DataFrame(query_point[4:132])\n",
    "                df_transposed = query_to_data.transpose()\n",
    "\n",
    "                kneighbor=knn.kneighbors(df_transposed,k)\n",
    "                # run all over the k nearest neighbors and extract from every point it's x,y,distance and index in orer to add it to the set\n",
    "                for i in range(k):\n",
    "                    y=kneighbor[0].iloc[i,0]\n",
    "                    x=kneighbor[0].iloc[i,1]\n",
    "                    dis=kneighbor[1][i]\n",
    "                    index=kneighbor[0].iloc[i,2]\n",
    "                    cor=(y,x)\n",
    "                    knn_tuple=(cor,dis,index)\n",
    "\n",
    "                    set_tuples.add(knn_tuple)\n",
    "                \n",
    "                \n",
    "            sorted_tuples = sorted(set_tuples, key=lambda x: x[1])\n",
    "\n",
    "            return sorted_tuples[:k]\n",
    "                \n",
    "    def find_closeset(self,set_tuples,group,query_point,k):\n",
    "\n",
    "        df = pd.DataFrame(group)\n",
    "        \n",
    "        df=df.iloc[:,4:132]\n",
    "        #If the cut of a table is empty\n",
    "        if group is None:\n",
    "            return\n",
    "        \n",
    "        #Calculates the the distances by the 128 rlevant features between the query point and each point of the cut\\leaf (LSH\\RDKT) by\n",
    "        distances = np.linalg.norm(df.values - query_point[4:132], axis=1)\n",
    "        if len(distances)>1:\n",
    "            #the logic of the line\"[:min(k,len(distances))]\" is: if k is bigger than the amount of points in the cut we will return all the points if the cut.\n",
    "            #otherwise we will choose the top k nearest points in the cut\n",
    "            \n",
    "            top_k_indices = np.argpartition(distances, min(k,len(distances)-1))[:min(k,len(distances))]\n",
    "            top_k_distances = distances[top_k_indices]\n",
    "            #Extract the x,y,distance and index and add it to the set\n",
    "            for i in top_k_indices:\n",
    "                point_y=group[i][0]\n",
    "                point_x=group[i][1]\n",
    "                point_index=group[i][132]\n",
    "                tuple_point=((point_y,point_x),distances[i],point_index)\n",
    "                set_tuples.add(tuple_point)\n",
    "        if len(distances)==1:\n",
    "            \n",
    "            point_y=group[0][0]\n",
    "            point_x=group[0][1]\n",
    "            point_index=group[0][132]\n",
    "            tuple_point=((point_y,point_x),distances[0],point_index)\n",
    "            set_tuples.add(tuple_point)\n",
    "            \n",
    "    #Method  for the classification, which by a given query point it provides the picture of the point\n",
    "    def predict(self,query_point,k):\n",
    "        #Use this method to get the top-k nearest neighbors of the query-point\n",
    "        nearest_list=self.find_k_neighbores(query_point,k)\n",
    "        hananya=0\n",
    "        hashmal=0\n",
    "        #Run all over th top-k nearest point\n",
    "        #Check the label of the neighbors and classify the query point by the majority\n",
    "        for cur_tuple in nearest_list:\n",
    "            #this is the main reson to save the index of the point befor - cur_tuple [2] provides the index of the point,and by this we can get the label of the point  \n",
    "            if self.target.iloc[int(cur_tuple[2]),2]=='hananya':\n",
    "                hananya+=1\n",
    "                \n",
    "            if self.target.iloc[int(cur_tuple[2]),2]=='hashmal':\n",
    "                hashmal+=1\n",
    "                \n",
    "        if(hananya>hashmal):\n",
    "            return 'hananya'\n",
    "        else:\n",
    "            return 'hashmal'\n",
    "            \n",
    "        \n",
    "    def ratio(self,query_point):\n",
    "        \n",
    "        top2_closeset=self.find_k_neighbores(query_point,2)\n",
    "        \n",
    "        #Find_k_neigbors might return only 1 point in case we put k=2, because if the k is bigger than the amount of the points in the cut, we return all the points in cut only.\n",
    "        #Based on this, the amount of points in cut could be 1 and then the method will return 1 point\n",
    "        #this \"if\" deal with this case by using KNN to get the 2 nearest point\n",
    "        if len(top2_closeset)==1:\n",
    "            \n",
    "            top2_closeset.clear()\n",
    "            knn=KNN()\n",
    "            knn.fit(self.data,self.data)\n",
    "            knn_list=knn.kneighbors(query_point[4:132],2)\n",
    "            \n",
    "            top2_closeset.append(((knn_list[0].iloc[0,0],knn_list[0].iloc[0,1]),knn_list[1][0]))\n",
    "            top2_closeset.append(((knn_list[0].iloc[1,0],knn_list[0].iloc[1,1]),knn_list[1][1]))\n",
    "        \n",
    "                       \n",
    "        #calculating the ratio, and return it as well as the cordinate of the nearest point and the distance\n",
    "        \n",
    "        value1=top2_closeset[0][1]\n",
    "        value2=top2_closeset[1][1]\n",
    "        if(value1>value2):\n",
    "            ratio=value2/value1\n",
    "            nearest_dis=value2\n",
    "            nearest_cordinates=top2_closeset[1][0]\n",
    "        if(value2>value1):\n",
    "            ratio=value1/value2\n",
    "            nearest_dis=value1\n",
    "            nearest_cordinates=top2_closeset[0][0]\n",
    "        if(ratio<0.8):\n",
    "            return nearest_dis,nearest_cordinates,ratio\n",
    "        else:\n",
    "            return nearest_dis,None,ratio\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "class KNN:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def fit(self,X,y=None):\n",
    "        \n",
    "        self.data=X.iloc[:,4:132]\n",
    "       \n",
    "        #this if reffers to the case we use this algorithm for classification. in this case we must keep the label of every point and therfor we return the 132 column which posses it. \n",
    "        #the else is used for every other use of the method besides classification in which we dont need to keep the labels of every point\n",
    "        if X.shape[1]==134:\n",
    "        \n",
    "            self.target=X.iloc[:,[0,1,132]]\n",
    "        else:\n",
    "            self.target=X.iloc[:,[0,1]]\n",
    "        \n",
    "    #This method calculate the distance between the query point and all the points in our data in order to select the top k nearest. it return data-frame(target) and numpy-array(distances)    \n",
    "    def kneighbors(self,new_record,k=1):\n",
    "        distances = np.linalg.norm(self.data.values - new_record.values, axis=1)\n",
    "        top_k_indices = np.argpartition(distances, k)[:k]\n",
    "        top_k_distances = distances[top_k_indices]\n",
    "        top_k_targets = self.target.iloc[top_k_indices]\n",
    "           \n",
    "        return top_k_targets, top_k_distances\n",
    "\n",
    "\n",
    "\n",
    "#Instead of reuse knn.kneighbors which will provide the same answer always, we run it one time and keep the results in a list\n",
    "\n",
    "def knn_time(data_set,data_test,k=1):\n",
    "    knn=KNN()\n",
    "    knn.fit(data_set)\n",
    "\n",
    "    start=time.time()\n",
    "    for i in range(len(data_test)):\n",
    "        knnlist.append(knn.kneighbors(data_test.iloc[i,4:132],k))\n",
    "    end=time.time()\n",
    "    return (\"knn\",end-start)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Method that calculates the error of a given ann and data-test by comparing it's results to the knn results\n",
    "\n",
    "def error_check(ann,data_test):\n",
    "    error=0\n",
    "    for i in range(len(data_test)):\n",
    "        current_knn_dis=float(knnlist[i][1])\n",
    "        current_ann_dis=ann.find_k_neighbores(data_test.iloc[i,:],1)[0][1]\n",
    "        error+=float(current_ann_dis/current_knn_dis-1)\n",
    "    return(error/len(data_test))\n",
    "\n",
    "\n",
    "\n",
    "#Generates values for RKDT and LSH and return the best tuple of(L,N) by chosing the one with least error\n",
    "def generate_ln(algo,data_set,data_test):\n",
    "    if algo=='RKDT':\n",
    "        for l in range(1,4,2):\n",
    "            for n in range(5,150,35):\n",
    "                tupel_ln=(l,n)\n",
    "                ann=ANN('RKDT',L=l,leaf_size=n)\n",
    "                ann.fit(data_set)\n",
    "                start=time.time()\n",
    "                error=error_check(ann,data_test)\n",
    "                end=time.time()\n",
    "                tupel_error=(tupel_ln,error,end-start)\n",
    "                error_ln_time_list.append(tupel_error)\n",
    "\n",
    "\n",
    "        return min(error_ln_time_list, key=lambda x: x[1])\n",
    "    \n",
    "    if algo=='LSH':\n",
    "        for l in range(3,6,2):\n",
    "            for n in range(5,26,5):\n",
    "                \n",
    "                tupel_ln=(l,n)\n",
    "                ann=ANN('LSH',L=l,num_cuts=n)\n",
    "                ann.fit(data_set)\n",
    "                start=time.time()\n",
    "                error=error_check(ann,data_test)\n",
    "                end=time.time()\n",
    "                tupel_error=(tupel_ln,error,end-start)\n",
    "                error_ln_time_list.append(tupel_error)\n",
    "                \n",
    "        return min(error_ln_time_list, key=lambda x: x[1])\n",
    "    \n",
    "\n",
    "\n",
    "#Use generate_ln method to get the best L,N in order to create a new ANN of this parameters\n",
    "#Provides the top 10 points from the data-test which have the least ratio and thier nearest point from the data-set\n",
    "def topk_ratio(data_set,data_test,tuple_ln,k=10,algo='LSH'):\n",
    "    \n",
    "    #tuple_ln=generate_ln(algo,data_set,data_test)[0]\n",
    "    if algo=='LSH':\n",
    "        ann=ANN(algo,L=tuple_ln[0],num_cuts=tuple_ln[1])\n",
    "    else:\n",
    "        ann=ANN(algo,L=tuple_ln[0],leaf_size=tuple_ln[1])\n",
    "        \n",
    "    ann.fit(data_set)\n",
    "    list_tuple=[]\n",
    "    for i in range(len(data_test)):\n",
    "        ratio_tuple=ann.ratio(data_test.iloc[i,:])\n",
    "        current_tuple=ratio_tuple+((data_test.iloc[i,0],data_test.iloc[i,1]),)\n",
    "        list_tuple.append(current_tuple)\n",
    "    sorted_list = sorted(list_tuple, key=lambda x: x[2])\n",
    "    my_list=sorted_list[:k]\n",
    "    \n",
    "    return [(x[1], x[3]) for x in my_list]\n",
    "\n",
    "#Use topk_ratio method to get the \"best\" points and mark them on the pictures\n",
    "\n",
    "def mark_picture(algorithm, data_set, data_test,optimal_pair,image1, image2):\n",
    "    \n",
    "    points_list=topk_ratio(data_set,data_test,optimal_pair,10,algo=algorithm)\n",
    "    img1 = Image.open(image1)\n",
    "    img2=Image.open(image2)\n",
    "    draw1 = ImageDraw.Draw(img1)\n",
    "    draw2 = ImageDraw.Draw(img2)\n",
    "    radius = 4\n",
    "    for tuple_point in points_list:\n",
    "        for i in range(10):\n",
    "            color = (random.randint(0, 255), random.randint(0, 255), random.randint(0, 255))\n",
    "            draw1.ellipse((tuple_point[0][1]-radius, tuple_point[0][0]-radius, tuple_point[0][1]+radius, tuple_point[0][0]+radius), fill=color)\n",
    "            draw2.ellipse((tuple_point[1][1]-radius, tuple_point[1][0]-radius, tuple_point[1][1]+radius, tuple_point[1][0]+radius), fill=color)\n",
    "    if len(data_set)>4000:\n",
    "        img1.save(\"marked_hashmal1.jpg\")\n",
    "        img2.save(\"marked_hashmal2.jpg\")\n",
    "    else:\n",
    "        img1.save(\"marked_hananya1.jpg\")\n",
    "        img2.save(\"marked_hananya2.jpg\")\n",
    "        \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "#Plot that represent the run-time per parameter\n",
    "#IMPORTANT-in order to use the plot methods We need to run first the generate_ln method which create the list we use here.\n",
    "def runtime_plot():\n",
    "    x_list=[]\n",
    "    y_list=[]\n",
    "    x_list.append(knn_tuple[0])\n",
    "    y_list.append(knn_tuple[1])\n",
    "    for i in range(len(error_ln_time_list)):\n",
    "        x_list.append(error_ln_time_list[i][0])\n",
    "        y_list.append(error_ln_time_list[i][2])\n",
    "    fig =plt.figure(figsize=(8, 6))\n",
    "    plt.bar(range(len(x_list)), y_list,width=0.6)\n",
    "    plt.xticks(range(len(x_list)), x_list)\n",
    "    plt.xlabel('KNN&ANN Algorithms', fontsize=10)\n",
    "    plt.ylabel('Run-Time(Seconds)', fontsize=10)\n",
    "    plt.title('Comparation of run-time between KNN and ANN with different Hyper-Parameters (L,leaf size/num cuts - according to the chosen algorithm) ', fontsize=10)\n",
    "    \n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "\n",
    "#Plot that represent the error rate per parameter\n",
    "#IMPORTANT-in order to use the plot methods We need to run first the generate_ln method which create the list we use here.\n",
    "def accuracy_plot():\n",
    "    x_list=[]\n",
    "    y_list=[]\n",
    "    for i in range(len(error_ln_time_list)):\n",
    "        x_list.append(error_ln_time_list[i][0])\n",
    "        y_list.append(error_ln_time_list[i][1])\n",
    "        \n",
    "    fig =plt.figure(figsize=(8, 6))\n",
    "    plt.bar(range(len(x_list)), y_list,width=0.6)\n",
    "    plt.xticks(range(len(x_list)), x_list)\n",
    "    plt.xlabel('ANN Algorithms', fontsize=10)\n",
    "    plt.ylabel('Error rate', fontsize=10)\n",
    "    plt.title('Comparation of error rates between ANN with different Hyper-Parameters (L,leaf size/num cuts - according to the chosen algorithm) ', fontsize=10)    \n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "leaf_size_list = [1,5,35,65, 95,125]\n",
    "\n",
    "#By given query point and a ANN algorithm from sklearn library the method calculate the distance from the nearest point\n",
    "def get_nearest_point(new_point,nbrs):\n",
    "    new_point = np.array(new_point).reshape(1, -1)\n",
    "    \n",
    "    # Find the nearest neighbor of the new_point using the trained model\n",
    "    distances, indices = nbrs.kneighbors(new_point)\n",
    "    \n",
    "    # Return the nearest point\n",
    "    return distances\n",
    "\n",
    "#Calculate the error for the sklearn model\n",
    "def error_cal(nbrs,data_test):\n",
    "    error=0\n",
    "    for i in range(len(data_test)):\n",
    "        current_knn_dis=float(knnlist[i][1])\n",
    "        current_nbrs=get_nearest_point(data_test.iloc[i,4:132].values.reshape(1, -1),nbrs)[0][0]\n",
    "        error+=float(current_nbrs/current_knn_dis-1)\n",
    "    return(error/len(data_test))\n",
    "\n",
    "#Examine the error and run time of the ANN algorithm of sklearn with different parameters and return the one with the least error.\n",
    "#If the errors equal the method return the one with runs faster\n",
    "def grid_search(data_set,data_test):\n",
    "    \n",
    "    min_tuple=()\n",
    "    data_set=data_set.iloc[:,4:132]\n",
    "    for size in leaf_size_list:\n",
    "        cur_leaf_size=size\n",
    "        nbrs = NearestNeighbors(n_neighbors=1,leaf_size=cur_leaf_size, algorithm='kd_tree',metric='minkowski',p=2, metric_params=None, n_jobs=None)\n",
    "        nbrs.fit(data_set.values)\n",
    "        start=time.time()\n",
    "        cur_error=error_cal(nbrs,data_test)\n",
    "        \n",
    "        end=time.time()\n",
    "        cur_time=end-start\n",
    "        cur_tuple=(cur_error,cur_time,cur_leaf_size)\n",
    "        if len(min_tuple)==0:\n",
    "            min_tuple=cur_tuple\n",
    "        if(cur_tuple[0]<min_tuple[0]):\n",
    "            min_tuple=cur_tuple\n",
    "        if(cur_tuple[0]==min_tuple[0]):\n",
    "            if(cur_tuple[1]<min_tuple[1]):\n",
    "                min_tuple=cur_tuple\n",
    "    return min_tuple\n",
    "\n",
    "\n",
    "\n",
    "#Comparation of error rate between sklearn ANN and My ANN with the best hyper-parameter(L,Leaf size)\n",
    "def sklear_accuracy_comp(data_set,data_test,optimal_pair):\n",
    "    x_list=[]\n",
    "    y_list=[]\n",
    "    sklearn_name='sklearn with \\n leaf size: '+str(grid_search(data_set,data_test)[2])\n",
    "    x_list.append(sklearn_name)\n",
    "    x_list.append(optimal_pair[0])\n",
    "    y_list.append(grid_search(data_set,data_test)[0])\n",
    "    y_list.append(optimal_pair[1])\n",
    "    \n",
    "    fig =plt.figure(figsize=(5, 5))\n",
    "    plt.bar(range(len(x_list)), y_list,width=0.3)\n",
    "    plt.xticks(range(len(x_list)), x_list)\n",
    "    plt.xlabel('Sklearn and ANN algorithms', fontsize=10)\n",
    "    plt.ylabel('Error rate', fontsize=10)\n",
    "    plt.title('Comparation of error rate between sklearn ANN and My ANN with the best hyper-parameter(L,Leaf size) ', fontsize=10)\n",
    "    \n",
    "\n",
    "    \n",
    "    plt.show()\n",
    "#Comparation of run time between sklearn ANN and My ANN with the best hyper-parameter(L,Leaf size)\n",
    "\n",
    "def sklearn_runtime_comp(data_set,data_test,optimal_pair):\n",
    "    \n",
    "    x_list=[]\n",
    "    y_list=[]\n",
    "    sklearn_name='sklearn with \\n leaf size: '+str(grid_search(data_set,data_test)[2])\n",
    "    x_list.append(sklearn_name)\n",
    "    x_list.append(optimal_pair[0])\n",
    "    y_list.append(grid_search(data_set,data_test)[1])\n",
    "    y_list.append(optimal_pair[2])\n",
    "    \n",
    "    fig =plt.figure(figsize=(5, 5))\n",
    "    plt.bar(range(len(x_list)), y_list,width=0.3)\n",
    "    plt.xticks(range(len(x_list)), x_list)\n",
    "    plt.xlabel('Sklearn and ANN algorithms', fontsize=10)\n",
    "    plt.ylabel('Run time (seconds)', fontsize=10)\n",
    "    plt.title('Comparation of run time between sklearn ANN and My ANN with the best hyper-parameter(L,Leaf size) ', fontsize=10)\n",
    "    \n",
    "\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "#Add a label column to the data-set and data-test which posses a string with the name of the picture they belong to\n",
    "#Combine the 2 data-sets to one.(same for data-tests)\n",
    "def combine(df1,df2):\n",
    "\n",
    "    df1['Label'] = 'hananya'\n",
    "    df2['Label']='hashmal'\n",
    "    combined = pd.concat([df1, df2])\n",
    "    return combined\n",
    "    \n",
    "\n",
    "#For each point in the combined data-test, the method associate a label according to the majority of the nearest points in the data-set\n",
    "def classification(algo,L,N,k,data_set1,data_set2,data_test1,data_test2):\n",
    "    \n",
    "    combined=combine(data_set1.copy(),data_set2.copy())\n",
    "    combined2=combine(data_test1.copy(),data_test2.copy())\n",
    "    if algo=='RKDT':\n",
    "        ann=ANN('RKDT',L=L,leaf_size=N)\n",
    "        ann.fit(combined.iloc[:,0:132],combined.iloc[:,[0,1,132]])\n",
    "    if algo=='LSH':\n",
    "        ann=ANN('LSH',L=L,num_cuts=N)\n",
    "        ann.fit(combined.iloc[:,0:132],combined.iloc[:,[0,1,132]])\n",
    "    correct=0\n",
    "    \n",
    "    for i in range(len(combined2)):\n",
    "        \n",
    "        if ann.predict(combined2.iloc[i,0:132],k)==combined2.iloc[i,132]:\n",
    "            correct+=1\n",
    "            \n",
    "    return correct/len(combined2)\n",
    "\n",
    "#num_cuts=the number of dimantions we use to split the data(LSH)\n",
    "#cuts_dict=Dictionary which posses a key as a hash-vector and value as a list of the points which have the same hash-vector\n",
    "#list_featues=contains tuples of (split dimention,random value)\n",
    "class LSH:\n",
    "    def __init__(self,num_cuts,data):\n",
    "        self.num_cuts=num_cuts\n",
    "        self.cuts_dict={}\n",
    "        self.data=data\n",
    "        self.list_features=[]\n",
    "        self.build_LSH()\n",
    "        \n",
    "\n",
    "    def build_LSH(self):\n",
    "        #Adding a column to the data-set that contains the hash vector(for every point)\n",
    "        self.data['Hash Vector']= '0' * self.num_cuts\n",
    "        arr=self.data.to_numpy()\n",
    "        n, m = arr.shape\n",
    "        list_featurs=[]\n",
    "        for i in range(self.num_cuts):\n",
    "            #Chose a random dimention, and a random value in the range of the values in this chosen dimention\n",
    "            split_dim = np.random.randint(4,m-2)\n",
    "            max_value = np.max(arr[:, split_dim]) \n",
    "            random_value = np.random.uniform(low=0.0, high=max_value)\n",
    "            tuple_split=(split_dim,random_value)\n",
    "            self.list_features.append(tuple_split)\n",
    "        #For each point we calculate its hash-vector and put it in as a key in the dictionary\n",
    "        for point in arr:\n",
    "            point[m-1]=self.point_hash(point)\n",
    "            \n",
    "        #Add every point to the list of the points which have the same hash-vector        \n",
    "        for point in arr:\n",
    "            if point[m-1] in self.cuts_dict:\n",
    "                self.cuts_dict[point[m-1]].append(point)\n",
    "            else:\n",
    "                self.cuts_dict[point[m-1]]=[]\n",
    "                self.cuts_dict[point[m-1]].append(point)\n",
    "                \n",
    "        for key,value in self.cuts_dict.items():\n",
    "            self.cuts_dict[key]=np.stack(self.cuts_dict[key])\n",
    "            \n",
    "            self.cuts_dict[key]=self.cuts_dict[key].astype(np.float64)\n",
    "            \n",
    "    #Method which gets a point and by the selected dimention and selected value provide a hash-vector for the point\n",
    "    #The first split is the first integer in the hash-vector and so in going on through all the dimentions\n",
    "    #if the current point has a bigger value than the random chosen value for the spesific dimention, the hash get '0', otherwise it gets 1\n",
    "    \n",
    "    def point_hash(self,point):\n",
    "        \n",
    "        hash_string=[]\n",
    "        for tuple_split in self.list_features:\n",
    "            if point[tuple_split[0]]<tuple_split[1]:\n",
    "                hash_string.append('0')\n",
    "            else:\n",
    "                hash_string.append('1')\n",
    "            \n",
    "        return ''.join(hash_string)\n",
    "    #By a given point this method finds it's hash-vector and by that, it finds the points which have the same hash-vector\n",
    "    def find_table(self,query):\n",
    "        \n",
    "        hash_string=self.point_hash(query)\n",
    "        cut=self.cuts_dict.get(hash_string)\n",
    "        \n",
    "        return cut\n",
    "    \n",
    "print('#----------------------For hananyas data-set------------------------#')\n",
    "\n",
    "#Part B.3:\n",
    "#knnlist keeps for each point in the data-test the nearest point in the data-set and the distance between them        \n",
    "knnlist=[]\n",
    "#knn_time return the time of the k-neighbors of the knn algorithm, and add to knnlist objects as described above\n",
    "\n",
    "knn_tuple=knn_time(hananya1,hananya2)\n",
    "print('Created a KNN list complited in '+str(knn_tuple[1])+' seconds')\n",
    "\n",
    "print('#----ALL OF THIS IS FOR RKDT----------#')\n",
    "#----ALL OF THIS IS FOR RKDT----------#\n",
    "#Part C.4:\n",
    "#This list contains all (chosen (L,N), thier error, their run-time)\n",
    "error_ln_time_list=[]\n",
    "#Generates values for RKDT and LSH and return the best tuple of(L,N) by chosing the one with least error\n",
    "optimal_pair=generate_ln('RKDT',hananya1,hananya2)\n",
    "print('The error and run-time of every pair of (L,N): ')\n",
    "print(error_ln_time_list)\n",
    "print('The pair with the optimal error: '+str(optimal_pair))\n",
    "\n",
    "\n",
    "#Part C.5:\n",
    "mark_picture('RKDT',hananya1,hananya2,optimal_pair[0],'Hananya1.JPG','Hananya2.JPG')\n",
    "print('marked images created succesfully')\n",
    "#Part C.6:\n",
    "runtime_plot()\n",
    "accuracy_plot()\n",
    "\n",
    "#Part D:\n",
    "\n",
    "sklearn_runtime_comp(hananya1,hananya2,optimal_pair)\n",
    "sklear_accuracy_comp(hananya1,hananya2,optimal_pair)\n",
    "\n",
    "#Part E:\n",
    "start=time.time()\n",
    "prediction_rate=classification('RKDT',5,150,115,hananya1,hashmal1,hananya2,hashmal2)\n",
    "print('Classifiaction prediction rate:'+str(prediction_rate))\n",
    "end=time.time()\n",
    "print(end-start)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#----ALL OF THIS IS FOR LSH----------#\n",
    "print('#----ALL OF THIS IS FOR LSH----------#')\n",
    "#Part C.4:\n",
    "#This list contains all (chosen (L,N), thier error, their run-time)\n",
    "error_ln_time_list=[]\n",
    "#Generates values for RKDT and LSH and return the best tuple of(L,N) by chosing the one with least error\n",
    "optimal_pair=generate_ln('LSH',hananya1,hananya2)\n",
    "print('The error and run-time of every pair of (L,N): ')\n",
    "print(error_ln_time_list)\n",
    "print('The pair with the optimal error: '+str(optimal_pair))\n",
    "\n",
    "\n",
    "#Part C.5:\n",
    "mark_picture('LSH',hananya1,hananya2,optimal_pair[0],'Hananya1.JPG','Hananya2.JPG')\n",
    "print('marked images created succesfully')\n",
    "#Part C.6:\n",
    "runtime_plot()\n",
    "accuracy_plot()\n",
    "\n",
    "#Part D:\n",
    "\n",
    "sklearn_runtime_comp(hananya1,hananya2,optimal_pair)\n",
    "sklear_accuracy_comp(hananya1,hananya2,optimal_pair)\n",
    "\n",
    "#Part E:\n",
    "start=time.time()\n",
    "prediction_rate=classification('LSH',5,20,70,hananya1,hashmal1,hananya2,hashmal2)\n",
    "\n",
    "print('Classifiaction prediction rate:'+str(prediction_rate))\n",
    "end=time.time()\n",
    "print(end-start)\n",
    "    \n",
    "    \n",
    "print('#----------------------For Hashmals data-set------------------------#')\n",
    "#Part B.3:\n",
    "#knnlist keeps for each point in the data-test the nearest point in the data-set and the distance between them        \n",
    "knnlist=[]\n",
    "#knn_time return the time of the k-neighbors of the knn algorithm, and add to knnlist objects as described above\n",
    "knn_tuple=knn_time(hashmal1,hashmal2)\n",
    "print('Created a KNN list complited in '+str(knn_tuple[1])+' seconds')\n",
    "\n",
    "print('#----ALL OF THIS IS FOR RKDT----------#')\n",
    "#----ALL OF THIS IS FOR RKDT----------#\n",
    "#Part C.4:\n",
    "#This list contains all (chosen (L,N), thier error, their run-time)\n",
    "error_ln_time_list=[]\n",
    "#Generates values for RKDT and LSH and return the best tuple of(L,N) by chosing the one with least error\n",
    "optimal_pair=generate_ln('RKDT',hashmal1,hashmal2)\n",
    "print('The error and run-time of every pair of (L,N): ')\n",
    "print(error_ln_time_list)\n",
    "print('The pair with the optimal error: '+str(optimal_pair))\n",
    "\n",
    "\n",
    "#Part C.5:\n",
    "mark_picture('RKDT',hashmal1,hashmal2,optimal_pair[0],'Hashmal1.jpg','Hashmal2.jpg')\n",
    "print('marked images created succesfully')\n",
    "#Part C.6:\n",
    "runtime_plot()\n",
    "accuracy_plot()\n",
    "\n",
    "#Part D:\n",
    "\n",
    "sklearn_runtime_comp(hashmal1,hashmal2,optimal_pair)\n",
    "sklear_accuracy_comp(hashmal1,hashmal2,optimal_pair)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#----ALL OF THIS IS FOR LSH----------#\n",
    "print('#----ALL OF THIS IS FOR LSH----------#')\n",
    "#Part C.4:\n",
    "#This list contains all (chosen (L,N), thier error, their run-time)\n",
    "error_ln_time_list=[]\n",
    "#Generates values for RKDT and LSH and return the best tuple of(L,N) by chosing the one with least error\n",
    "optimal_pair=generate_ln('LSH',hashmal1,hashmal2)\n",
    "print('The error and run-time of every pair of (L,N): ')\n",
    "print(error_ln_time_list)\n",
    "print('The pair with the optimal error: '+str(optimal_pair))\n",
    "\n",
    "\n",
    "#Part C.5:\n",
    "mark_picture('LSH',hashmal1,hashmal2,optimal_pair[0],'Hashmal1.jpg','Hashmal2.jpg')\n",
    "print('marked images created succesfully')\n",
    "#Part C.6:\n",
    "runtime_plot()\n",
    "accuracy_plot()\n",
    "\n",
    "#Part D:\n",
    "\n",
    "sklearn_runtime_comp(hashmal1,hashmal2,optimal_pair)\n",
    "sklear_accuracy_comp(hashmal1,hashmal2,optimal_pair)\n",
    "\n",
    " \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd0086a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088489b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
